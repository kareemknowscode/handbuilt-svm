{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b7d9756",
   "metadata": {},
   "source": [
    "# A Handbuilt Soft-SVM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6415177f",
   "metadata": {},
   "source": [
    "## Creating the soft-SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa821fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def classify(train_words, train_labels, test_words):\n",
    "    \"\"\"\n",
    "    Classify the given test words into 'spanish' or 'french' based on the training data.\n",
    "\n",
    "    Parameters:\n",
    "    train_words (list of str): List of words in the training set.\n",
    "    train_labels (list of str): List of labels ('spanish' or 'french') for the training set.\n",
    "    test_words (list of str): List of words in the test set to classify.\n",
    "\n",
    "    Returns:\n",
    "    list of str: Predicted labels ('spanish' or 'french') for the test set.\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame({\"word\": train_words, \"label\": train_labels})\n",
    "    data['onehot_spanish'] = data['label'].apply(lambda x: 1 if x == 'spanish' else -1)\n",
    "\n",
    "    class NGramTokenizer:\n",
    "        \"\"\"\n",
    "        Tokenizer to generate n-grams from words and create a vocabulary of n-grams.\n",
    "        \"\"\"\n",
    "        def __init__(self, ngram_range):\n",
    "            \"\"\"\n",
    "            Initialize the NGramTokenizer with the specified range of n-grams.\n",
    "\n",
    "            Parameters:\n",
    "            ngram_range (tuple): Tuple specifying the lower and upper bounds for n-grams.\n",
    "            \"\"\"\n",
    "            self.ngram_range = ngram_range\n",
    "            self.vocab = defaultdict(int)\n",
    "\n",
    "        def generate_ngrams(self, word):\n",
    "            \"\"\"\n",
    "            Generate n-grams for the given word based on the specified n-gram range.\n",
    "\n",
    "            Parameters:\n",
    "            word (str): The word to generate n-grams from.\n",
    "\n",
    "            Returns:\n",
    "            list of str: List of generated n-grams.\n",
    "            \"\"\"\n",
    "            ngrams = []\n",
    "            lower, upper = self.ngram_range\n",
    "            for n in range(lower, upper + 1):\n",
    "                for i in range(len(word) - n + 1):\n",
    "                    ngrams.append(word[i:i + n])\n",
    "            return ngrams\n",
    "\n",
    "        def fit(self, words):\n",
    "            \"\"\"\n",
    "            Fit the tokenizer to the given list of words and create the n-gram vocabulary.\n",
    "\n",
    "            Parameters:\n",
    "            words (list of str): List of words to fit the tokenizer on.\n",
    "\n",
    "            Returns:\n",
    "            numpy.ndarray: Matrix of shape (number of words, size of vocabulary) representing\n",
    "                           the presence of n-grams in each word.\n",
    "            \"\"\"\n",
    "            index = 0\n",
    "            for word in words:\n",
    "                ngrams = self.generate_ngrams(word)\n",
    "                for ngram in ngrams:\n",
    "                    if ngram not in self.vocab:\n",
    "                        self.vocab[ngram] = index\n",
    "                        index += 1\n",
    "\n",
    "            X = np.zeros((len(words), len(self.vocab)))\n",
    "            for i, word in enumerate(words):\n",
    "                ngrams = self.generate_ngrams(word)\n",
    "                for ngram in ngrams:\n",
    "                    if ngram in self.vocab:\n",
    "                        X[i, self.vocab[ngram]] = 1\n",
    "\n",
    "            return X\n",
    "    \n",
    "    ngram = NGramTokenizer(ngram_range=(2, 5))\n",
    "    X_train = ngram.fit(data['word'])\n",
    "    pairs = list(ngram.vocab.keys())\n",
    "    \n",
    "    y_train = data['onehot_spanish'].to_numpy()\n",
    "\n",
    "    # Soft-SVM using Subgradient Descent\n",
    "    alpha = 0.01\n",
    "    C = 1  # Regularization parameter\n",
    "    lambda_ = 0.01  # Regularization parameter\n",
    "    w = np.zeros(len(X_train[0]))\n",
    "\n",
    "    for n in range(1000):\n",
    "        for i in range(len(X_train)):\n",
    "            if y_train[i] * np.dot(X_train[i], w) < 1:\n",
    "                w = w - alpha * (2 * w / len(X_train) - C * y_train[i] * X_train[i])\n",
    "            else:\n",
    "                subgradient = 2 * w / len(X_train) - lambda_ * w\n",
    "                if y_train[i] * np.dot(X_train[i], w) == 1:\n",
    "                    subgradient += -C * y_train[i] * X_train[i]\n",
    "                w = w - alpha * subgradient\n",
    "\n",
    "    def create_pattern_matrix(words, patterns):\n",
    "        \"\"\"\n",
    "        Create a pattern matrix for the given words and patterns.\n",
    "\n",
    "        Parameters:\n",
    "        words (list of str): List of words to create the pattern matrix for.\n",
    "        patterns (list of str): List of patterns (n-grams) to look for in the words.\n",
    "\n",
    "        Returns:\n",
    "        numpy.ndarray: Matrix of shape (number of words, number of patterns) representing\n",
    "                       the presence of patterns in each word.\n",
    "        \"\"\"\n",
    "        pattern_matrix = np.zeros((len(words), len(patterns)), dtype=float)\n",
    "        for i, word in enumerate(words):\n",
    "            for j, pattern in enumerate(patterns):\n",
    "                if pattern in word:\n",
    "                    pattern_matrix[i, j] = 1\n",
    "        return pattern_matrix\n",
    "\n",
    "    X_test = create_pattern_matrix(test_words, pairs)\n",
    "\n",
    "    # Predict the labels for the test set\n",
    "    predictions = []\n",
    "    for i in range(len(X_test)):\n",
    "        raw_prediction = np.dot(X_test[i], w)\n",
    "        predicted_label = 'spanish' if raw_prediction >= 0 else 'french'\n",
    "        predictions.append(predicted_label)\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a5ebcd",
   "metadata": {},
   "source": [
    "## Example usage of the SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2529b397",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "new_words = [\n",
    "    \"amigo\", \"bonjour\", \"casa\", \"ecole\", \"libro\", \"chien\", \"coche\", \"fromage\", \n",
    "    \"mesa\", \"jardin\", \"mar\", \"soleil\", \"rapido\", \"fleur\", \"familia\", \"ville\", \n",
    "    \"musica\", \"lait\", \"comida\", \"rouge\", \"playa\", \"neige\", \"salud\", \"ami\", \n",
    "    \"verde\", \"livre\", \"gato\", \"pomme\", \"escuela\", \"eau\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "testing = classify(train['word'], train['label'], new_words)  \n",
    "\n",
    "new_labels = [\n",
    "    \"spanish\", \"french\", \"spanish\", \"french\", \"spanish\", \"french\", \"spanish\", \"french\", \n",
    "    \"spanish\", \"french\", \"spanish\", \"french\", \"spanish\", \"french\", \"spanish\", \"french\", \n",
    "    \"spanish\", \"french\", \"spanish\", \"french\", \"spanish\", \"french\", \"spanish\", \"french\", \n",
    "    \"spanish\", \"french\", \"spanish\", \"french\", \"spanish\", \"french\"\n",
    "]\n",
    "\n",
    "assert len(testing) == len(new_labels)\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = len(testing)\n",
    "for predicted_label, actual_label in zip(testing, new_labels):\n",
    "    if predicted_label == actual_label:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
